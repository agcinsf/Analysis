{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc0e8c6",
   "metadata": {},
   "source": [
    "##### This code takes a file from Twitter (they give a 1% data feed for free) and runs analytics on it in python.  It also have MapReduce examples below and a SQLite example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "532669b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import collections\n",
    "\n",
    "#def TweetPrint(term,pages):\n",
    "#    for n in range(pages-1):\n",
    "##        response = urllib.urlopen(\"http://search.twitter.com/search.json?q=\"+ term + \"&page=\" + str(n+1))\n",
    " #       a = json.load(response)\n",
    " #       for i in a['results']: \n",
    " #           print '%s' % i['text']\n",
    "\n",
    "            \n",
    "def ReadDictionary(fileLoc):\n",
    "    final=[]\n",
    "    with open(fileLoc, \"r\") as handle:\n",
    "        for line in handle.read().split('\\n'):\n",
    "            for i in xrange(len(''.join(line).split())-1):\n",
    "                a = len(''.join(line).split())\n",
    "                final.append([''.join(line).split()[i],float(''.join(line).split()[a-1])])\n",
    "    return dict(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd1a094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 52.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "sent_file = 'C:/Users/ac/Google Drive/DS/assignment1/AFINN-111.txt'\n",
    "\n",
    "%timeit s=ReadDictionary(sent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb1318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù†Ø§Øª\t0.000\n",
      "Eu:httpâ€¦\t0.000\n",
      "don't\t0.000\n",
      "Guess\t0.000\n",
      "Melissaaa\t0.000\n",
      "RT\t0.000\n",
      "Giuli\t0.000\n",
      "le\t0.000\n",
      "hable\t0.000\n",
      "#NP\t0.000\n",
      "la\t0.000\n",
      "Ju\t0.000\n",
      ",\t0.000\n",
      "dÃ¡\t0.000\n",
      "to\t0.000\n",
      "Kaum\t0.000\n",
      "Not\t0.000\n",
      "te\t0.000\n",
      "disfarÃ§a,\t0.000\n",
      "sÃ³\t0.000\n",
      "his\t0.136\n",
      "amo\t0.000\n",
      "nenem\t0.000\n",
      "dan\t0.000\n",
      "who's\t0.000\n",
      "Ø¬Ù†Ø³\t0.000\n",
      "Ø³ÙƒØ³\t0.000\n",
      "da\t0.000\n",
      "petit\t0.000\n",
      "@___BenJ:\t0.136\n",
      "hands\t0.136\n",
      "like\t-0.300\n",
      "d\t0.000\n",
      "sabrosa\t0.000\n",
      "Ø¯Ù„ÙˆØ¹Ø§Øª\t0.000\n",
      "?\t0.000\n",
      "x\t0.000\n",
      "Pemimpin\t0.000\n",
      "el\t0.000\n",
      "everyone\t0.000\n",
      "partying\t0.000\n",
      "hari\t0.000\n",
      "people\t0.000\n",
      "the\t-0.300\n",
      "@_RatedRobin:\t0.000\n",
      "Ø§ÙˆÙ„Ø§\t0.000\n",
      "jamais\t0.000\n",
      "semua\t0.000\n",
      "out\t0.136\n",
      "flirt\t0.000\n",
      "NÃ£o\t0.000\n",
      "Triples\t0.000\n",
      "giving\t0.000\n",
      "+\t0.000\n",
      "agacha\t0.000\n",
      "crush\t0.000\n",
      "@DoaIndah:\t0.000\n",
      "@AfterPuberty:\t0.000\n",
      "sampai\t0.000\n",
      "&gt;&gt;\t0.000\n",
      "hermosa\t0.000\n",
      "can\t-0.300\n",
      "be\t0.000\n",
      "Punchlinovic\t0.000\n",
      "ã‚‚ã�—ãƒ„ã‚¤ãƒ¼ãƒˆè¦‹ã‚‰ã‚Œã�Ÿã‚‰ã�Šã�¾ã‚“ã�“ã�³ã�—ã‚ƒã�³ã�—ã‚ƒ\t0.000\n",
      "tinggikanlah\t0.000\n",
      "Mi\t0.000\n",
      "wear\t-0.300\n",
      "sexy\t0.000\n",
      "&amp;\t0.000\n",
      "cara\t0.000\n",
      "come\t0.136\n",
      "http://t.co/ZFxVrOZruy\t0.000\n",
      "cuerpo\t0.000\n",
      "round\t0.000\n",
      "arrivera\t0.000\n",
      "getting\t0.000\n",
      "@alwaysl0ve1d:\t0.000\n",
      "rika\t0.000\n",
      "anos\t0.000\n",
      "Bachelorette\t0.000\n",
      "kalimat-Mu\t0.000\n",
      "Zayn:\"Gosto\t0.000\n",
      "sophia\t0.000\n",
      "http://t.co/Xmoa9u5tmv\t0.000\n",
      "jeito\t0.000\n",
      "ðŸ”«âœ‹ðŸ˜¤ðŸ˜¡\t0.000\n",
      "arrepio,\t0.000\n",
      "own\t0.136\n",
      "drunk\t0.000\n",
      "C'Ã©tait\t0.000\n",
      "app\t0.000\n",
      "When\t0.000\n",
      "Le\t0.000\n",
      "arrepios,\t0.000\n",
      "your\t0.000\n",
      "tiene\t0.000\n",
      "ØªØ¹Ø§Ø±Ù�\t0.000\n",
      "Ã©\t0.000\n",
      ")))):\t0.000\n",
      "nem\t0.000\n",
      "all\t-0.300\n",
      "lol\t0.136\n",
      "carinha\t0.000\n",
      "Dinos\t0.000\n",
      "much\t0.136\n",
      "@Luka_Albaares\t0.000\n",
      "mama\t0.000\n",
      "can't\t-0.300\n",
      "Louis:\"dps\t0.000\n",
      "that\t0.000\n",
      "Kardashian\t0.000\n",
      "bitches\t-0.300\n",
      "reageeer\t0.000\n",
      "rubs\t0.136\n",
      "dios\t0.000\n",
      "1!\t0.000\n",
      "â™¥\t0.000\n",
      "with\t0.136\n",
      "seguire\t0.000\n",
      "he\t0.136\n",
      "me\t0.000\n",
      "Tqt\t0.000\n",
      "13\t0.000\n",
      "look\t0.000\n",
      "https://t.co/puh1l7SJ2P\t0.000\n",
      "Allah\t0.000\n",
      "Ø§Ù„Ø¹Ø±Ø¨\t0.000\n",
      "http://t.co/CosRcz6VoP\t0.000\n",
      "n\t0.000\n",
      "um\t0.000\n",
      "Ø§Ù„Ø§Ù†\t0.000\n",
      "@breminaj_x:\t0.136\n",
      "del\t0.000\n",
      "Khloe\t0.000\n",
      "AAAAAAAAH\t0.000\n",
      "@ibarra_franklin:\t0.000\n",
      "@SucksToSucks:\t0.000\n",
      "kiamatâ€¦\t0.000\n",
      "Pas\t0.000\n",
      "perbaiki\t0.000\n",
      "Muslimin,\t0.000\n",
      "is\t0.000\n",
      "sanitizer\t0.136\n",
      "mundo\t0.000\n",
      "Ø§Ø±ØªØ¨Ø§Ø·\t0.000\n",
      "tonight\t0.000\n",
      "Nekfeu\t0.000\n",
      "as\t0.136\n",
      "http://t.co/7eHw2Zxq\t0.000\n",
      "need\t0.136\n",
      "Party\t0.000\n",
      "tan\t-0.300\n",
      "durs\t0.000\n",
      "!\t0.000\n",
      "sa\t0.000\n",
      "Ø§Ù†Ø¶Ù…\t0.000\n",
      "pas\t0.000\n",
      "!!!\t0.000\n",
      "Ø²ÙˆØ§Ø¬\t0.000\n",
      "Taff\t0.000\n",
      "Ya\t0.000\n",
      "na\t0.000\n",
      "normal.\t0.000\n",
      "-\t0.000\n",
      "Lmaooo\t0.136\n",
      "nuca...\t0.000\n",
      "Zayn\"\t0.000\n",
      "&lt;&lt;\t0.000\n",
      "@voutestuprei:\t0.000\n",
      "@FrankOceaan:\t0.000\n",
      "Umat\t0.000\n",
      "mas\t0.000\n",
      "de\t0.000\n",
      "ft\t0.000\n",
      "rukunkanlah\t0.000\n",
      "eres\t0.000\n",
      "I\t-0.300\n",
      "haha\t0.000\n",
      "BouchÃ©es\t0.000\n",
      "Birdman\t0.136\n",
      "carinho\t0.000\n",
      "@hrap4love:\t0.000\n",
      "â€¦\t0.000\n",
      "mal\t0.000\n",
      "a\t0.000\n",
      "â€“\t0.000\n",
      "e\t0.000\n",
      "&lt;3\t0.000\n",
      "sala\t0.000\n",
      "@Marife_Navarro\t0.000\n",
      "q\t0.000\n",
      "Mddr\t0.000\n",
      "cochon\t0.000\n",
      "besotes\t0.000\n",
      "Bitch\t0.000\n",
      "dos\t0.000\n",
      "bom..\"\t0.000\n",
      "@fcbmelissx\t0.000\n",
      "Ø±ÙŠØªÙˆÙŠØª\t0.000\n",
      "Islam\t0.000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "g = {}\n",
    "with open('C:/Users/aclark/Google Drive/DS/assignment1/smallTest.txt','r') as tweets:\n",
    "    for tweet in tweets.read().split('\\n'):\n",
    "        sumVal = 0\n",
    "        value = 0\n",
    "        jsonData = json.loads(tweet)\n",
    "        if 'created_at' in jsonData.keys():\n",
    "             value = sum([int(s.get(t.encode('utf-8'),0)) for t in jsonData['text'].split()])\n",
    "        else:\n",
    "             value = 0\n",
    "    \n",
    "    #Score the tweet\n",
    "        for t in re.findall(\"[^\\s]+\",jsonData['text']):\n",
    "            score = 0\n",
    "            #loop through the words in the tweet to see if any exist in the sentiment file\n",
    "            if t.encode('utf-8') in s.keys():\n",
    "                #if they exist check whether they are positive or negative\n",
    "                score += float(s[t.encode('utf-8')])/len(jsonData['text'].split())\n",
    "            # if positive the tweet is positive else it is negative\n",
    "            else:\n",
    "                #add the text to the sentiment file with the sentiment of 0\n",
    "                g[t.encode('utf-8')] = float(0)\n",
    "        \n",
    "        # Armed with the scores we can now assign each word the score of the tweet\n",
    "        for t in jsonData['text'].split():\n",
    "            g[t.encode('utf-8')] = score\n",
    "           \n",
    "for key,val in g.iteritems():\n",
    "    print '%s\\t%.3f' % (key,float(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483ce2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù†Ø§Øª\t0.000\n",
      "Eu:httpâ€¦\t0.000\n",
      "don't\t0.000\n",
      "Guess\t-2.000\n",
      "Melissaaa\t0.000\n",
      "RT\t6.000\n",
      "Giuli\t0.000\n",
      "le\t3.000\n",
      "hable\t0.000\n",
      "#NP\t0.000\n",
      "la\t3.000\n",
      "Ju\t0.000\n",
      ",\t0.000\n",
      "dÃ¡\t0.000\n",
      "to\t3.000\n",
      "Kaum\t0.000\n",
      "Not\t0.000\n",
      "te\t6.000\n",
      "disfarÃ§a,\t0.000\n",
      "sÃ³\t0.000\n",
      "his\t6.000\n",
      "amo\t3.000\n",
      "nenem\t0.000\n",
      "dan\t0.000\n",
      "who's\t-2.000\n",
      "Ø¬Ù†Ø³\t0.000\n",
      "Ø³ÙƒØ³\t0.000\n",
      "da\t0.000\n",
      "petit\t3.000\n",
      "@___BenJ:\t3.000\n",
      "hands\t3.000\n",
      "like\t-3.000\n",
      "d\t0.000\n",
      "sabrosa\t3.000\n",
      "Ø¯Ù„ÙˆØ¹Ø§Øª\t0.000\n",
      "?\t3.000\n",
      "x\t3.000\n",
      "Pemimpin\t0.000\n",
      "el\t3.000\n",
      "everyone\t0.000\n",
      "partying\t-2.000\n",
      "hari\t0.000\n",
      "people\t-1.000\n",
      "the\t-3.000\n",
      "@_RatedRobin:\t0.000\n",
      "Ø§ÙˆÙ„Ø§\t0.000\n",
      "jamais\t3.000\n",
      "semua\t0.000\n",
      "out\t3.000\n",
      "flirt\t-1.000\n",
      "NÃ£o\t0.000\n",
      "Triples\t0.000\n",
      "giving\t0.000\n",
      "+\t0.000\n",
      "agacha\t0.000\n",
      "crush\t-1.000\n",
      "@DoaIndah:\t0.000\n",
      "@AfterPuberty:\t0.000\n",
      "sampai\t0.000\n",
      "&gt;&gt;\t0.000\n",
      "hermosa\t3.000\n",
      "can\t-3.000\n",
      "be\t0.000\n",
      "Punchlinovic\t0.000\n",
      "ã‚‚ã�—ãƒ„ã‚¤ãƒ¼ãƒˆè¦‹ã‚‰ã‚Œã�Ÿã‚‰ã�Šã�¾ã‚“ã�“ã�³ã�—ã‚ƒã�³ã�—ã‚ƒ\t0.000\n",
      "tinggikanlah\t0.000\n",
      "Mi\t0.000\n",
      "wear\t-3.000\n",
      "sexy\t3.000\n",
      "&amp;\t-2.000\n",
      "cara\t0.000\n",
      "come\t3.000\n",
      "http://t.co/ZFxVrOZruy\t0.000\n",
      "cuerpo\t3.000\n",
      "round\t0.000\n",
      "arrivera\t3.000\n",
      "getting\t-2.000\n",
      "@alwaysl0ve1d:\t0.000\n",
      "rika\t3.000\n",
      "anos\t0.000\n",
      "Bachelorette\t0.000\n",
      "kalimat-Mu\t0.000\n",
      "Zayn:\"Gosto\t0.000\n",
      "sophia\t0.000\n",
      "http://t.co/Xmoa9u5tmv\t-2.000\n",
      "jeito\t0.000\n",
      "ðŸ”«âœ‹ðŸ˜¤ðŸ˜¡\t-1.000\n",
      "arrepio,\t0.000\n",
      "own\t3.000\n",
      "drunk\t-2.000\n",
      "C'Ã©tait\t3.000\n",
      "app\t0.000\n",
      "When\t-1.000\n",
      "Le\t0.000\n",
      "arrepios,\t0.000\n",
      "your\t-1.000\n",
      "tiene\t3.000\n",
      "ØªØ¹Ø§Ø±Ù�\t0.000\n",
      "Ã©\t0.000\n",
      ")))):\t0.000\n",
      "nem\t0.000\n",
      "all\t-3.000\n",
      "lol\t3.000\n",
      "carinha\t0.000\n",
      "Dinos\t0.000\n",
      "much\t3.000\n",
      "@Luka_Albaares\t3.000\n",
      "mama\t0.000\n",
      "can't\t-3.000\n",
      "Louis:\"dps\t0.000\n",
      "that\t0.000\n",
      "Kardashian\t0.000\n",
      "bitches\t-3.000\n",
      "reageeer\t0.000\n",
      "rubs\t3.000\n",
      "dios\t3.000\n",
      "1!\t0.000\n",
      "â™¥\t0.000\n",
      "with\t2.000\n",
      "seguire\t3.000\n",
      "he\t3.000\n",
      "me\t0.000\n",
      "Tqt\t3.000\n",
      "13\t0.000\n",
      "look\t0.000\n",
      "https://t.co/puh1l7SJ2P\t0.000\n",
      "Allah\t0.000\n",
      "Ø§Ù„Ø¹Ø±Ø¨\t0.000\n",
      "http://t.co/CosRcz6VoP\t0.000\n",
      "n\t0.000\n",
      "um\t0.000\n",
      "Ø§Ù„Ø§Ù†\t0.000\n",
      "@breminaj_x:\t3.000\n",
      "del\t3.000\n",
      "Khloe\t0.000\n",
      "AAAAAAAAH\t0.000\n",
      "@ibarra_franklin:\t3.000\n",
      "@SucksToSucks:\t-2.000\n",
      "kiamatâ€¦\t0.000\n",
      "Pas\t0.000\n",
      "perbaiki\t0.000\n",
      "Muslimin,\t0.000\n",
      "is\t0.000\n",
      "sanitizer\t3.000\n",
      "mundo\t3.000\n",
      "Ø§Ø±ØªØ¨Ø§Ø·\t0.000\n",
      "tonight\t-2.000\n",
      "Nekfeu\t0.000\n",
      "as\t6.000\n",
      "http://t.co/7eHw2Zxq\t0.000\n",
      "need\t3.000\n",
      "Party\t0.000\n",
      "tan\t-3.000\n",
      "durs\t3.000\n",
      "!\t3.000\n",
      "sa\t3.000\n",
      "Ø§Ù†Ø¶Ù…\t0.000\n",
      "pas\t0.000\n",
      "!!!\t3.000\n",
      "Ø²ÙˆØ§Ø¬\t0.000\n",
      "Taff\t3.000\n",
      "Ya\t0.000\n",
      "na\t0.000\n",
      "normal.\t0.000\n",
      "-\t0.000\n",
      "Lmaooo\t3.000\n",
      "nuca...\t0.000\n",
      "Zayn\"\t0.000\n",
      "&lt;&lt;\t-1.000\n",
      "@voutestuprei:\t0.000\n",
      "@FrankOceaan:\t-1.000\n",
      "Umat\t0.000\n",
      "mas\t3.000\n",
      "de\t3.000\n",
      "ft\t0.000\n",
      "rukunkanlah\t0.000\n",
      "eres\t3.000\n",
      "I\t-3.000\n",
      "haha\t3.000\n",
      "BouchÃ©es\t0.000\n",
      "Birdman\t3.000\n",
      "carinho\t0.000\n",
      "@hrap4love:\t0.000\n",
      "â€¦\t3.000\n",
      "mal\t0.000\n",
      "a\t-3.000\n",
      "â€“\t0.000\n",
      "e\t0.000\n",
      "&lt;3\t0.000\n",
      "sala\t0.000\n",
      "@Marife_Navarro\t3.000\n",
      "q\t3.000\n",
      "Mddr\t3.000\n",
      "cochon\t3.000\n",
      "besotes\t3.000\n",
      "Bitch\t0.000\n",
      "dos\t0.000\n",
      "bom..\"\t0.000\n",
      "@fcbmelissx\t0.000\n",
      "Ø±ÙŠØªÙˆÙŠØª\t0.000\n",
      "Islam\t0.000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def ReadDictionary(fileLoc):\n",
    "    with open(fileLoc, \"r\") as handle:\n",
    "         lookup = dict((x[0], x[1]) for x in (x.split('\\t') for x in handle.read().split('\\n') if x))\n",
    "         return lookup\n",
    "\n",
    "def main():\n",
    "    g = {}\n",
    "    s=ReadDictionary('C:/Users/Ac/Google Drive/DS/assignment1/AFINN-111.txt')\n",
    "    \n",
    "    with open('C:/Users/Ac/Google Drive/DS/assignment1/smallTest.txt','r') as tweets:\n",
    "        for tweet in tweets.read().split('\\n'):\n",
    "            jsonData = json.loads(tweet)\n",
    "            value = sum([int(s[t]) for t in re.findall(\"[^\\s]+\",jsonData.get('text')) if t.encode('utf-8') in s.keys()])\n",
    "                \n",
    "            #add the missing tweet words to the g dictionary\n",
    "            for t in re.findall(\"[^\\s]+\",jsonData['text']):\n",
    "                if not t.encode('utf-8') in g.keys():\n",
    "                     g[t.encode('utf-8')] = value\n",
    "                else:\n",
    "                     g[t.encode('utf-8')]  += value\n",
    "        \n",
    "        for key,val in g.iteritems():\n",
    "            print '%s\\t%.3f' % (key,float(val))\n",
    "                    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff4dc3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xd8\\xa8\\xd9\\x86\\xd8\\xa7\\xd8\\xaa', 'Eu:http\\xe2\\x80\\xa6', \"don't\", 'Guess', 'Melissaaa', 'RT', 'Giuli', 'le', 'hable', '#NP', 'la', 'Ju', ',', 'd\\xc3\\xa1', 'to', 'Kaum', 'Not', 'te', 'disfar\\xc3\\xa7a,', 's\\xc3\\xb3', 'his', 'amo', 'nenem', 'dan', \"who's\", '\\xd8\\xac\\xd9\\x86\\xd8\\xb3', '\\xd8\\xb3\\xd9\\x83\\xd8\\xb3', 'da', 'petit', '@___BenJ:', 'hands', 'like', 'd', 'sabrosa', '\\xd8\\xaf\\xd9\\x84\\xd9\\x88\\xd8\\xb9\\xd8\\xa7\\xd8\\xaa', '?', 'x', 'Pemimpin', 'el', 'everyone', 'partying', 'hari', 'people', 'the', '@_RatedRobin:', '\\xd8\\xa7\\xd9\\x88\\xd9\\x84\\xd8\\xa7', 'jamais', 'semua', 'out', 'flirt', 'N\\xc3\\xa3o', 'Triples', 'giving', '+', 'agacha', 'crush', '@DoaIndah:', '@AfterPuberty:', 'sampai', '&gt;&gt;', 'hermosa', 'can', 'be', 'Punchlinovic', '\\xe3\\x82\\x82\\xe3\\x81\\x97\\xe3\\x83\\x84\\xe3\\x82\\xa4\\xe3\\x83\\xbc\\xe3\\x83\\x88\\xe8\\xa6\\x8b\\xe3\\x82\\x89\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xe3\\x82\\x89\\xe3\\x81\\x8a\\xe3\\x81\\xbe\\xe3\\x82\\x93\\xe3\\x81\\x93\\xe3\\x81\\xb3\\xe3\\x81\\x97\\xe3\\x82\\x83\\xe3\\x81\\xb3\\xe3\\x81\\x97\\xe3\\x82\\x83', 'tinggikanlah', 'Mi', 'wear', 'sexy', '&amp;', 'cara', 'come', 'http://t.co/ZFxVrOZruy', 'cuerpo', 'round', 'arrivera', 'getting', '@alwaysl0ve1d:', 'rika', 'anos', 'Bachelorette', 'kalimat-Mu', 'Zayn:\"Gosto', 'sophia', 'http://t.co/Xmoa9u5tmv', 'jeito', '\\xf0\\x9f\\x94\\xab\\xe2\\x9c\\x8b\\xf0\\x9f\\x98\\xa4\\xf0\\x9f\\x98\\xa1', 'arrepio,', 'own', 'drunk', \"C'\\xc3\\xa9tait\", 'app', 'When', 'Le', 'arrepios,', 'your', 'tiene', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xa7\\xd8\\xb1\\xd9\\x81', '\\xc3\\xa9', ')))):', 'nem', 'all', 'lol', 'carinha', 'Dinos', 'much', '@Luka_Albaares', 'mama', \"can't\", 'Louis:\"dps', 'that', 'Kardashian', 'bitches', 'reageeer', 'rubs', 'dios', '1!', '\\xe2\\x99\\xa5', 'with', 'seguire', 'he', 'me', 'Tqt', '13', 'look', 'https://t.co/puh1l7SJ2P', 'Allah', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8', 'http://t.co/CosRcz6VoP', 'n', 'um', '\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x86', '@breminaj_x:', 'del', 'Khloe', 'AAAAAAAAH', '@ibarra_franklin:', '@SucksToSucks:', 'kiamat\\xe2\\x80\\xa6', 'Pas', 'perbaiki', 'Muslimin,', 'is', 'sanitizer', 'mundo', '\\xd8\\xa7\\xd8\\xb1\\xd8\\xaa\\xd8\\xa8\\xd8\\xa7\\xd8\\xb7', 'tonight', 'Nekfeu', 'as', 'http://t.co/7eHw2Zxq', 'need', 'Party', 'tan', 'durs', '!', 'sa', '\\xd8\\xa7\\xd9\\x86\\xd8\\xb6\\xd9\\x85', 'pas', '!!!', '\\xd8\\xb2\\xd9\\x88\\xd8\\xa7\\xd8\\xac', 'Taff', 'Ya', 'na', 'normal.', '-', 'Lmaooo', 'nuca...', 'Zayn\"', '&lt;&lt;', '@voutestuprei:', '@FrankOceaan:', 'Umat', 'mas', 'de', 'ft', 'rukunkanlah', 'eres', 'I', 'haha', 'Bouch\\xc3\\xa9es', 'Birdman', 'carinho', '@hrap4love:', '\\xe2\\x80\\xa6', 'mal', 'a', '\\xe2\\x80\\x93', 'e', '&lt;3', 'sala', '@Marife_Navarro', 'q', 'Mddr', 'cochon', 'besotes', 'Bitch', 'dos', 'bom..\"', '@fcbmelissx', '\\xd8\\xb1\\xd9\\x8a\\xd8\\xaa\\xd9\\x88\\xd9\\x8a\\xd8\\xaa', 'Islam']\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    g = {}\n",
    "    totalGs = 0\n",
    "    with open('C:/Users/aclark/Google Drive/DS/assignment1/smallTest.txt','r') as tweets:\n",
    "        for tweet in tweets.read().split('\\n'):\n",
    "            jsonData = json.loads(tweet)\n",
    "            for t in jsonData['text'].split():\n",
    "                if not t in g.keys():\n",
    "                    g[t.encode('utf-8')] = 1\n",
    "                    totalGs +=1\n",
    "        \n",
    "        print g.keys()\n",
    "       \n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6319e8",
   "metadata": {},
   "source": [
    "# Happiest State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a14dce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL\n",
      "         321437 function calls (318959 primitive calls) in 12.495 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.000    0.000 :0(charmap_decode)\n",
      "        2    0.000    0.000    0.000    0.000 :0(decode)\n",
      "      294    0.003    0.000    0.003    0.000 :0(encode)\n",
      "    69376    0.330    0.000    0.330    0.000 :0(end)\n",
      "      127    0.001    0.000    0.001    0.000 :0(get)\n",
      "        2    0.000    0.000    0.000    0.000 :0(isinstance)\n",
      "        1    0.000    0.000    0.000    0.000 :0(iteritems)\n",
      "    35044    0.241    0.000    0.241    0.000 :0(keys)\n",
      "    35066    0.138    0.000    0.138    0.000 :0(len)\n",
      "    69376    0.556    0.000    0.556    0.000 :0(match)\n",
      "        1    0.000    0.000    0.000    0.000 :0(max)\n",
      "        2    0.000    0.000    0.000    0.000 :0(open)\n",
      "        1    0.000    0.000    0.000    0.000 :0(read)\n",
      "        1    0.000    0.000    0.000    0.000 :0(setprofile)\n",
      "     2983    0.015    0.000    0.015    0.000 :0(split)\n",
      "        2    0.000    0.000    0.000    0.000 :0(time)\n",
      "        2    0.000    0.000    0.000    0.000 :0(write)\n",
      "      127    0.003    0.000    0.006    0.000 <ipython-input-40-893a5de91fbe>:12(scoreTweet)\n",
      "        1    2.025    2.025   12.494   12.494 <ipython-input-40-893a5de91fbe>:16(main)\n",
      "        1    0.016    0.016    0.086    0.086 <ipython-input-40-893a5de91fbe>:7(ReadDictionary)\n",
      "4956/2478    0.058    0.000    0.069    0.000 <ipython-input-40-893a5de91fbe>:9(<genexpr>)\n",
      "        1    0.000    0.000   12.494   12.494 <string>:1(<module>)\n",
      "    34688    0.483    0.000   10.129    0.000 __init__.py:281(loads)\n",
      "        2    0.000    0.000    0.000    0.000 cp1252.py:14(decode)\n",
      "    34688    1.386    0.000    9.647    0.000 decoder.py:361(decode)\n",
      "    34688    7.238    0.000    7.238    0.000 decoder.py:372(raw_decode)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:66(write)\n",
      "        1    0.000    0.000   12.495   12.495 profile:0(main())\n",
      "        0    0.000             0.000          profile:0(profiler)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "import operator\n",
    "import profile\n",
    "\n",
    "def ReadDictionary(fileLoc):\n",
    "     with open(fileLoc, \"r\") as handle:\n",
    "        lookup = dict((x[0], x[1]) for x in (x.split('\\t') for x in handle.read().split('\\n') if x))\n",
    "        return lookup\n",
    "\n",
    "def scoreTweet(text,sent_file):\n",
    "    for t in text.split():\n",
    "        return float(sent_file.get(t.encode('utf-8'),0))\n",
    "        \n",
    "def main():\n",
    "    stateList = ['AL','AK','AZ','AR','CA','CO','CT','DE','DC','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "    states={}\n",
    "    smallNumber = 0\n",
    "    i =0\n",
    "    #open the sent_file and read into a dictionary and tweet file\n",
    "    sent_file = ReadDictionary('C:/Users/Ac/Google Drive/DS/assignment1/AFINN-111.txt')\n",
    "    tweets = open('C:/Users/Ac/Google Drive/DS/assignment1/output.txt')\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        jsonData = json.loads(tweet)\n",
    "        if 'place' in jsonData.keys() and jsonData['place'] is not None and jsonData['place']['country_code']=='US' and 'text' in jsonData.keys():\n",
    "            state = jsonData['place']['full_name'].split()[len(jsonData['place']['full_name'].split())-1]\n",
    "            if len(state) == 2 and state in stateList:\n",
    "                if state not in states.keys():\n",
    "                    states[state.encode('utf-8')] = 0\n",
    "                else:\n",
    "                    states[state.encode('utf-8')] += scoreTweet(jsonData['text'],sent_file)\n",
    "    #find the biggest value and return the key\n",
    "    print max(states.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    profile.run('main()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbed2f",
   "metadata": {},
   "source": [
    "# Hashtag Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3785e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueCadaCaprilistaTengaUnMillonDeSeguidores\t8.000000\n",
      "デリヘル\t1.000000\n",
      "ANDROIDGAMES\t1.000000\n",
      "OusadiaeAlegriaNoMultishow\t1.000000\n",
      "ExpressionDeBabtou\t1.000000\n",
      "ピックアップ\t1.000000\n",
      "DenverMeetup\t1.000000\n",
      "OusadiaEAlegriaNoMultishow\t1.000000\n",
      "Coyoacan\t1.000000\n",
      "senyu\t1.000000\n",
      "CPU times: user 0.12 s, sys: 0.00 s, total: 0.12 s\n",
      "Wall time: 0.12 s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "def ReadDictionary(fileLoc):\n",
    "     with open(fileLoc, \"r\") as handle:\n",
    "        lookup = dict((x[0], x[1]) for x in (x.split('\\t') for x in handle.read().split('\\n') if x))\n",
    "        return lookup\n",
    "\n",
    "def main():\n",
    "    i =0\n",
    "    htag = {}\n",
    "    #open the sent_file and read into a dictionary and tweet file\n",
    "    tweets = open('C:/Users/Ac/Google Drive/DS/assignment1/smallTest.txt')\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        jsonData = json.loads(tweet)\n",
    "        if 'entities' in jsonData.keys() and len(jsonData['entities']['hashtags'])>0:\n",
    "            for i in jsonData['entities']['hashtags']:\n",
    "                htag[i['text']] = htag.get(i['text'],0) + 1\n",
    "                \n",
    "    #loop throught the htag and print the top 10\n",
    "    sorted_x = sorted(htag.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    for i,j in enumerate(sorted_x):\n",
    "        if i <= 9:\n",
    "            print '%s\\t%f' % (j[0],j[1])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    %time main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7a899",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14a136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'10130_txt_earn', 1), (u'10209_txt_trade', 1), (u'10213_txt_acq', 1), (u'1022_txt_trade', 1), (u'10255_txt_trade', 1), (u'10264_txt_trade', 1), (u'10455_txt_trade', 1), (u'10462_txt_acq', 1), (u'10530_txt_trade', 1), (u'10545_txt_interest', 1), (u'10608_txt_trade', 1), (u'1078_txt_interest', 1), (u'10883_txt_acq', 1), (u'10905_txt_trade', 1), (u'11076_txt_trade', 1), (u'11225_txt_trade', 1), (u'11357_txt_trade', 1), (u'11461_txt_trade', 1), (u'11555_txt_trade', 1), (u'11558_txt_trade', 1), (u'11574_txt_trade', 1), (u'11580_txt_trade', 1), (u'11658_txt_trade', 1), (u'11721_txt_earn', 1), (u'11746_txt_interest', 1), (u'11755_txt_acq', 1), (u'11771_txt_trade', 1), (u'11916_txt_acq', 1), (u'11941_txt_interest', 1), (u'12010_txt_interest', 1), (u'12016_txt_interest', 1), (u'1201_txt_trade', 1), (u'12040_txt_interest', 1), (u'12073_txt_interest', 1), (u'12250_txt_acq', 1), (u'1226_txt_trade', 1), (u'12305_txt_acq', 1), (u'12333_txt_interest', 1), (u'12397_txt_earn', 1), (u'12457_txt_trade', 1), (u'12473_txt_trade', 1), (u'12563_txt_trade', 1), (u'12604_txt_trade', 1), (u'12609_txt_crude', 1), (u'12616_txt_earn', 1), (u'12623_txt_trade', 1), (u'12629_txt_acq', 1), (u'12880_txt_interest', 1), (u'12908_txt_interest', 1), (u'12912_txt_interest', 1), (u'12919_txt_interest', 1), (u'13008_txt_interest', 1), (u'13024_txt_interest', 1), (u'13039_txt_trade', 1), (u'13085_txt_acq', 1), (u'13182_txt_acq', 1), (u'13194_txt_acq', 1), (u'13212_txt_acq', 1), (u'1343_txt_crude', 1), (u'1354_txt_acq', 1), (u'1356_txt_trade', 1), (u'13908_txt_trade', 1), (u'13946_txt_trade', 1), (u'14012_txt_trade', 1), (u'14199_txt_trade', 1), (u'14270_txt_interest', 1), (u'14623_txt_acq', 1), (u'14739_txt_trade', 1), (u'1477_txt_acq', 1), (u'14881_txt_trade', 1), (u'14912_txt_trade', 1), (u'14982_txt_acq', 1), (u'1499_txt_trade', 1), (u'15154_txt_trade', 1), (u'15171_txt_trade', 1), (u'15223_txt_trade', 1), (u'15227_txt_earn', 1), (u'15276_txt_acq', 1), (u'1541_txt_earn', 1), (u'15447_txt_trade', 1), (u'15560_txt_interest', 1), (u'15578_txt_interest', 1), (u'15579_txt_interest', 1), (u'15600_txt_acq', 1), (u'15610_txt_interest', 1), (u'15617_txt_interest', 1), (u'15816_txt_interest', 1), (u'1581_txt_acq', 1), (u'15894_txt_acq', 1), (u'16029_txt_acq', 1), (u'16095_txt_trade', 1), (u'16274_txt_interest', 1), (u'16354_txt_earn', 1), (u'16442_txt_trade', 1), (u'16505_txt_trade', 1), (u'16651_txt_crude', 1), (u'16684_txt_acq', 1), (u'16685_txt_acq', 1), (u'16700_txt_acq', 1), (u'16774_txt_trade', 1), (u'1677_txt_earn', 1), (u'16787_txt_trade', 1), (u'16788_txt_trade', 1), (u'16794_txt_trade', 1), (u'1679_txt_acq', 1), (u'1684_txt_acq', 1), (u'16872_txt_acq', 1), (u'16874_txt_acq', 1), (u'16926_txt_trade', 1), (u'16929_txt_trade', 1), (u'16932_txt_trade', 1), (u'16939_txt_crude', 1), (u'16968_txt_crude', 1), (u'16970_txt_trade', 1), (u'16981_txt_interest', 1), (u'16989_txt_interest', 1), (u'16995_txt_acq', 1), (u'17004_txt_acq', 1), (u'17025_txt_acq', 1), (u'17071_txt_interest', 1), (u'17074_txt_trade', 1), (u'17083_txt_trade', 1), (u'17115_txt_acq', 1), (u'17194_txt_trade', 1), (u'17269_txt_trade', 1), (u'17304_txt_trade', 1), (u'17359_txt_crude', 1), (u'17408_txt_crude', 1), (u'17431_txt_crude', 1), (u'17452_txt_trade', 1), (u'17651_txt_acq', 1), (u'17780_txt_crude', 1), (u'17816_txt_crude', 1), (u'18066_txt_crude', 1), (u'18104_txt_trade', 1), (u'18108_txt_crude', 1), (u'18138_txt_acq', 1), (u'18148_txt_trade', 1), (u'18182_txt_acq', 1), (u'18205_txt_acq', 1), (u'18257_txt_trade', 1), (u'18302_txt_trade', 1), (u'1835_txt_acq', 1), (u'1839_txt_trade', 1), (u'18554_txt_trade', 1), (u'1863_txt_trade', 1), (u'1871_txt_trade', 1), (u'18904_txt_interest', 1), (u'18986_txt_acq', 1), (u'19163_txt_trade', 1), (u'19377_txt_trade', 1), (u'19433_txt_trade', 1), (u'19441_txt_acq', 1), (u'19448_txt_acq', 1), (u'19483_txt_interest', 1), (u'1951_txt_trade', 1), (u'19658_txt_trade', 1), (u'19683_txt_acq', 1), (u'19713_txt_acq', 1), (u'19760_txt_acq', 1), (u'19880_txt_acq', 1), (u'20038_txt_interest', 1), (u'20159_txt_interest', 1), (u'20220_txt_earn', 1), (u'20286_txt_earn', 1), (u'20409_txt_interest', 1), (u'20457_txt_acq', 1), (u'20469_txt_trade', 1), (u'20766_txt_acq', 1), (u'20769_txt_interest', 1), (u'20787_txt_trade', 1), (u'20797_txt_acq', 1), (u'20837_txt_earn', 1), (u'20865_txt_trade', 1), (u'20917_txt_acq', 1), (u'20959_txt_crude', 1), (u'20989_txt_trade', 1), (u'21031_txt_trade', 1), (u'21071_txt_acq', 1), (u'21197_txt_crude', 1), (u'21228_txt_acq', 1), (u'2128_txt_acq', 1), (u'21357_txt_acq', 1), (u'21369_txt_crude', 1), (u'21408_txt_acq', 1), (u'21491_txt_interest', 1), (u'2175_txt_crude', 1), (u'2219_txt_acq', 1), (u'2228_txt_interest', 1), (u'2238_txt_earn', 1), (u'2268_txt_acq', 1), (u'2520_txt_acq', 1), (u'2538_txt_acq', 1), (u'2540_txt_acq', 1), (u'2552_txt_acq', 1), (u'2802_txt_acq', 1), (u'2806_txt_earn', 1), (u'2850_txt_earn', 1), (u'2862_txt_trade', 1), (u'2953_txt_acq', 1), (u'3009_txt_acq', 1), (u'3039_txt_acq', 1), (u'3078_txt_trade', 1), (u'3199_txt_trade', 1), (u'3204_txt_crude', 1), (u'3360_txt_interest', 1), (u'3417_txt_acq', 1), (u'342_txt_trade', 1), (u'3711_txt_acq', 1), (u'3902_txt_trade', 1), (u'3920_txt_acq', 1), (u'3931_txt_trade', 1), (u'3948_txt_acq', 1), (u'3980_txt_crude', 1), (u'3982_txt_interest', 1), (u'4026_txt_trade', 1), (u'4048_txt_trade', 1), (u'4055_txt_acq', 1), (u'4132_txt_earn', 1), (u'4340_txt_crude', 1), (u'4488_txt_acq', 1), (u'4510_txt_acq', 1), (u'4525_txt_crude', 1), (u'4552_txt_trade', 1), (u'4595_txt_trade', 1), (u'4600_txt_crude', 1), (u'4604_txt_crude', 1), (u'489_txt_crude', 1), (u'4903_txt_trade', 1), (u'4937_txt_acq', 1), (u'502_txt_crude', 1), (u'5057_txt_trade', 1), (u'5127_txt_interest', 1), (u'5235_txt_interest', 1), (u'5376_txt_trade', 1), (u'5469_txt_acq', 1), (u'5516_txt_acq', 1), (u'5561_txt_trade', 1), (u'5717_txt_trade', 1), (u'5750_txt_acq', 1), (u'5925_txt_earn', 1), (u'6091_txt_interest', 1), (u'6096_txt_acq', 1), (u'6119_txt_crude', 1), (u'6163_txt_crude', 1), (u'6169_txt_crude', 1), (u'6178_txt_interest', 1), (u'6223_txt_acq', 1), (u'6264_txt_crude', 1), (u'6301_txt_crude', 1), (u'6473_txt_acq', 1), (u'6507_txt_earn', 1), (u'6578_txt_crude', 1), (u'6746_txt_crude', 1), (u'6926_txt_trade', 1), (u'696_txt_earn', 1), (u'7003_txt_earn', 1), (u'7011_txt_trade', 1), (u'7097_txt_crude', 1), (u'718_txt_trade', 1), (u'7287_txt_crude', 1), (u'7305_txt_acq', 1), (u'7310_txt_interest', 1), (u'7319_txt_acq', 1), (u'7477_txt_trade', 1), (u'7521_txt_trade', 1), (u'7529_txt_crude', 1), (u'7600_txt_trade', 1), (u'7611_txt_crude', 1), (u'7626_txt_trade', 1), (u'7632_txt_trade', 1), (u'7733_txt_acq', 1), (u'7791_txt_acq', 1), (u'7813_txt_acq', 1), (u'7899_txt_acq', 1), (u'7997_txt_acq', 1), (u'8029_txt_trade', 1), (u'8044_txt_trade', 1), (u'8117_txt_crude', 1), (u'8134_txt_crude', 1), (u'8137_txt_interest', 1), (u'8209_txt_crude', 1), (u'844_txt_acq', 1), (u'8478_txt_crude', 1), (u'853_txt_acq', 1), (u'8554_txt_trade', 1), (u'8692_txt_trade', 1), (u'8699_txt_trade', 1), (u'8835_txt_crude', 1), (u'8951_txt_interest', 1), (u'9184_txt_trade', 1), (u'9253_txt_crude', 1), (u'925_txt_trade', 1), (u'9315_txt_trade', 1), (u'9415_txt_trade', 1), (u'9532_txt_trade', 1), (u'96_txt_acq', 1), (u'10355_txt_trade', 2), (u'10665_txt_trade', 2), (u'10781_txt_trade', 2), (u'11314_txt_interest', 2), (u'11390_txt_trade', 2), (u'11397_txt_trade', 2), (u'11444_txt_crude', 2), (u'11446_txt_trade', 2), (u'11455_txt_crude', 2), (u'11460_txt_trade', 2), (u'11487_txt_trade', 2), (u'11498_txt_trade', 2), (u'11545_txt_trade', 2), (u'12027_txt_interest', 2), (u'12278_txt_interest', 2), (u'12472_txt_trade', 2), (u'12564_txt_trade', 2), (u'12810_txt_interest', 2), (u'12846_txt_interest', 2), (u'13046_txt_trade', 2), (u'13144_txt_interest', 2), (u'14293_txt_interest', 2), (u'14826_txt_trade', 2), (u'14904_txt_trade', 2), (u'15352_txt_trade', 2), (u'15666_txt_trade', 2), (u'16088_txt_trade', 2), (u'16852_txt_interest', 2), (u'16856_txt_trade', 2), (u'16871_txt_trade', 2), (u'1694_txt_acq', 2), (u'17075_txt_trade', 2), (u'17201_txt_trade', 2), (u'17335_txt_trade', 2), (u'17758_txt_interest', 2), (u'18519_txt_trade', 2), (u'18835_txt_trade', 2), (u'18992_txt_trade', 2), (u'19291_txt_crude', 2), (u'19546_txt_trade', 2), (u'19915_txt_earn', 2), (u'20262_txt_interest', 2), (u'21187_txt_trade', 2), (u'2620_txt_trade', 2), (u'2648_txt_trade', 2), (u'2725_txt_interest', 2), (u'354_txt_trade', 2), (u'5160_txt_trade', 2), (u'5288_txt_trade', 2), (u'6598_txt_crude', 2), (u'6716_txt_trade', 2), (u'6764_txt_interest', 2), (u'7135_txt_trade', 2), (u'7715_txt_acq', 2), (u'7771_txt_trade', 2), (u'7802_txt_trade', 2), (u'7907_txt_trade', 2), (u'8252_txt_trade', 2), (u'8982_txt_trade', 2), (u'9015_txt_trade', 2), (u'9515_txt_trade', 2), (u'9603_txt_trade', 2), (u'9628_txt_trade', 2), (u'9657_txt_trade', 2), (u'9675_txt_interest', 2), (u'9697_txt_trade', 2), (u'10265_txt_trade', 3), (u'10352_txt_trade', 3), (u'10642_txt_trade', 3), (u'10767_txt_trade', 3), (u'10779_txt_trade', 3), (u'11175_txt_trade', 3), (u'1197_txt_trade', 3), (u'14220_txt_trade', 3), (u'16075_txt_interest', 3), (u'16745_txt_trade', 3), (u'16935_txt_trade', 3), (u'16957_txt_trade', 3), (u'17023_txt_trade', 3), (u'17256_txt_trade', 3), (u'17305_txt_trade', 3), (u'17449_txt_trade', 3), (u'17511_txt_trade', 3), (u'18520_txt_interest', 3), (u'1963_txt_trade', 3), (u'20632_txt_crude', 3), (u'3111_txt_interest', 3), (u'3534_txt_interest', 3), (u'3762_txt_interest', 3), (u'6201_txt_crude', 3), (u'12774_txt_interest', 4), (u'12848_txt_trade', 4), (u'16214_txt_interest', 4), (u'16681_txt_interest', 4), (u'1711_txt_crude', 4), (u'1809_txt_trade', 4), (u'18399_txt_trade', 4), (u'233_txt_interest', 4), (u'3371_txt_interest', 4), (u'9012_txt_interest', 4), (u'9795_txt_trade', 4), (u'10623_txt_trade', 5), (u'5964_txt_trade', 5), (u'16094_txt_trade', 6), (u'16357_txt_trade', 6), (u'19775_txt_interest', 6)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "db = 'C:/Users/Ac/Google Drive/DS/assignment2/reuters.db'\n",
    "db2 = 'C:/Users/Ac/Google Drive/DS/assignment2/matrix.db'\n",
    "\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql1 = 'SELECT Count() FROM frequency WHERE docid=\"10398_txt_earn\";'\n",
    "sql2 = 'SELECT Count() FROM frequency WHERE docid=\"10398_txt_earn\" and count=1;'\n",
    "sql3 = 'SELECT Count(DISTINCT term) From (SELECT * FROM frequency WHERE (docid=\"10398_txt_earn\" and count=1) UNION SELECT * FROM frequency WHERE (docid=\"925_txt_trade\" and count=1));'\n",
    "sql4 = 'SELECT count(DISTINCT docid) FROM frequency WHERE term=\"parliament\";'\n",
    "sql5 = 'SELECT count() FROM (SELECT docid, sum(count) as \"sCount\" FROM frequency GROUP BY docid) WHERE sCount>300;'\n",
    "sql6 = 'SELECT docid FROM frequency WHERE term=\"world\" INTERSECT SELECT docid FROM frequency WHERE term=\"transactions\";'\n",
    "sql7 = 'SELECT * FROM (SELECT A.row_num,B.col_num, SUM(A.value*B.value) FROM A,B WHERE A.col_num = B.row_num GROUP BY A.row_num,B.col_Num) WHERE row_num=2 and col_num=3'\n",
    "sql8 = 'SELECT sum(total) FROM (SELECT SUM(A.count*B.count) as \"total\" FROM frequency A, frequency B WHERE A.term = B.term and A.docid = \"10080_txt_crude\" and B.docid=\"17035_txt_earn\" GROUP BY A.docid,B.term)'\n",
    "sql9 = 'SELECT B.docid,sum(A.count*B.count) as \"total\" FROM (SELECT * FROM frequency UNION SELECT \"q\" as docid, \"washington\" as term, 1 as count UNION SELECT \"q\" as docid, \"taxes\" as term, 1 as count UNION SELECT \"q\" as docid, \"treasury\" as term, 1 as count) A, (SELECT * FROM frequency WHERE term=\"washington\" or term=\"taxes\" or term=\"treasury\") B WHERE A.term = B.term and A.docid=\"q\" GROUP BY B.docid ORDER BY total desc'\n",
    "\n",
    "%time cursor.execute(sql9)\n",
    "\n",
    "print cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816fb57",
   "metadata": {},
   "source": [
    "# MAPREDUCE        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1152747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class MapReduce:\n",
    "    def __init__(self):\n",
    "        self.intermediate = {}\n",
    "        self.result = []\n",
    "\n",
    "    def emit_intermediate(self, key, value):\n",
    "        self.intermediate.setdefault(key, [])\n",
    "        self.intermediate[key].append(value)\n",
    "\n",
    "    def emit(self, value):\n",
    "        self.result.append(value) \n",
    "\n",
    "    def execute(self, data, mapper, reducer):\n",
    "        for line in data:\n",
    "            mapper(self, line)\n",
    "\n",
    "        for key in self.intermediate:\n",
    "            reducer(self, key, self.intermediate[key])\n",
    "\n",
    "        jenc = json.JSONEncoder(encoding='latin-1')\n",
    "        for item in self.result:\n",
    "            print jenc.encode(item)\n",
    "\n",
    "def execute(data, mapper, reducer):\n",
    "    mr = MapReduce()\n",
    "    mr.execute(data, mapper, reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f39b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
